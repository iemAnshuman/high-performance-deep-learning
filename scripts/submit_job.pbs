#!/bin/bash

# --- Job Configuration ---
#PBS -N triton_158_train
#PBS -l nodes=4:ppn=8:gpus=1
#PBS -l walltime=48:00:00
#PBS -q gpu
#PBS -j oe
#PBS -o logs/training_run_${PBS_JOBID}.log

# --- Environment Variables (Edit these for your cluster) ---
# Example: /home/anshuman/miniconda3/envs/hpc_dl
ENV_PATH="./venv" 
PROJECT_DIR="$PBS_O_WORKDIR"

# ---------------------------------------------------------

echo "[Job] Starting on node: $(hostname)"
echo "[Job] ID: $PBS_JOBID"
echo "[Job] Directory: $PROJECT_DIR"

# 1. Navigate to Project
cd "$PROJECT_DIR" || { echo "Error: Could not cd to $PROJECT_DIR"; exit 1; }

# 2. Load Modules (University Clusters usually require this)
# using '|| true' to prevent crash if module isn't found (portability)
module load cuda/12.1 || echo "Warning: System CUDA module not loaded"
module load python/3.10 || echo "Warning: System Python module not loaded"
module load mpi/openmpi-4.1 || echo "Warning: MPI module not loaded"

# 3. Activate Environment (Robust Check)
if [ -f "$ENV_PATH/bin/activate" ]; then
    source "$ENV_PATH/bin/activate"
    echo "[Env] Venv activated."
elif [ -d "$ENV_PATH" ]; then
    # Assume Conda if bin/activate is missing but dir exists
    source $(conda info --base)/etc/profile.d/conda.sh
    conda activate "$ENV_PATH"
    echo "[Env] Conda env activated."
else
    echo "Error: Environment not found at $ENV_PATH"
    exit 1
fi

# 4. Debug: Print GPU and MPI Info
nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
which mpirun

# 5. Launch
echo "[Run] Launching Training..."
# Note: Use full path to python to avoid ambiguity
mpirun -np 4 $(which python) distributed/ring_reduce.cpp

echo "[Job] Finished at: $(date)"